{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ui4F3zJ33aAO",
        "outputId": "7e7ecf91-46f2-4297-dfc7-34017b4a4c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.43.1)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.14)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python numpy gradio mediapipe Pillow moviepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kNa-MIHZ3aAQ",
        "outputId": "2c8404af-7eea-4932-d5a4-a41ef5714c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gradio/interface.py:414: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://56b6328e24a0e95982.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://56b6328e24a0e95982.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 POSITIONS CIBLES FIXES:\n",
            "   Œil gauche: (570, 250)\n",
            "   Œil droit: (710, 250)\n",
            "   Écartement: 140px\n",
            "\n",
            "📸 === TRAITEMENT IMAGE 1/4 ===\n",
            "Fichier: Image 1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👁️  Détection: MediaPipe\n",
            "   Yeux actuels: L[812 581] R[935 610]\n",
            "🔄 Rotation nécessaire: -13.52°\n",
            "📏 Échelle appliquée: 1.110\n",
            "➡️  Translation: [-233 -345]\n",
            "✅ Image 1 alignée avec succès\n",
            "\n",
            "📸 === TRAITEMENT IMAGE 2/4 ===\n",
            "Fichier: Image 2.jpg\n",
            "👁️  Détection: Haar_Cascade\n",
            "   Yeux actuels: L[449 673] R[501 673]\n",
            "🔄 Rotation nécessaire: 0.00°\n",
            "📏 Échelle appliquée: 2.000\n",
            "➡️  Translation: [ 165 -423]\n",
            "✅ Image 2 alignée avec succès\n",
            "\n",
            "📸 === TRAITEMENT IMAGE 3/4 ===\n",
            "Fichier: Image 3.jpg\n",
            "👁️  Détection: MediaPipe\n",
            "   Yeux actuels: L[559 522] R[698 557]\n",
            "🔄 Rotation nécessaire: -14.03°\n",
            "📏 Échelle appliquée: 0.977\n",
            "➡️  Translation: [  11 -290]\n",
            "✅ Image 3 alignée avec succès\n",
            "\n",
            "📸 === TRAITEMENT IMAGE 4/4 ===\n",
            "Fichier: Image 4.jpg\n",
            "👁️  Détection: Haar_Cascade\n",
            "   Yeux actuels: L[633 559] R[747 559]\n",
            "🔄 Rotation nécessaire: 0.00°\n",
            "📏 Échelle appliquée: 1.223\n",
            "➡️  Translation: [ -50 -309]\n",
            "✅ Image 4 alignée avec succès\n",
            "\n",
            "🎬 CRÉATION VIDÉO avec 4 images\n",
            "💾 Sauvegarde de la vidéo...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import mediapipe as mp\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Initialisation\n",
        "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def detect_face_and_eyes(image_rgb):\n",
        "    \"\"\"\n",
        "    Détection robuste avec validation stricte\n",
        "    \"\"\"\n",
        "    h, w, _ = image_rgb.shape\n",
        "    img_for_detection = image_rgb.astype(np.uint8)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Méthode 1: MediaPipe - Plus précise\n",
        "    try:\n",
        "        rgb_image = cv2.cvtColor(img_for_detection, cv2.COLOR_BGR2RGB)\n",
        "        res = mp_face_mesh.process(rgb_image)\n",
        "\n",
        "        if res and res.multi_face_landmarks:\n",
        "            lm = res.multi_face_landmarks[0].landmark\n",
        "\n",
        "            # Points clés pour les yeux\n",
        "            left_eye_points = [\n",
        "                (lm[33].x * w, lm[33].y * h),   # Coin externe\n",
        "                (lm[133].x * w, lm[133].y * h), # Coin interne\n",
        "                (lm[160].x * w, lm[160].y * h), # Bas\n",
        "                (lm[158].x * w, lm[158].y * h), # Haut\n",
        "            ]\n",
        "\n",
        "            right_eye_points = [\n",
        "                (lm[362].x * w, lm[362].y * h), # Coin interne\n",
        "                (lm[263].x * w, lm[263].y * h), # Coin externe\n",
        "                (lm[385].x * w, lm[385].y * h), # Bas\n",
        "                (lm[387].x * w, lm[387].y * h), # Haut\n",
        "            ]\n",
        "\n",
        "            # Centre des yeux (moyenne des points)\n",
        "            left_eye = np.mean(left_eye_points, axis=0)\n",
        "            right_eye = np.mean(right_eye_points, axis=0)\n",
        "\n",
        "            # Validation géométrique\n",
        "            eye_distance = np.linalg.norm(right_eye - left_eye)\n",
        "            eye_center_y = (left_eye[1] + right_eye[1]) / 2\n",
        "\n",
        "            # Critères de validation\n",
        "            if (50 < eye_distance < w * 0.7 and  # Distance raisonnable\n",
        "                0.1 * h < eye_center_y < 0.7 * h):  # Position verticale réaliste\n",
        "\n",
        "                results.append({\n",
        "                    'left_eye': left_eye,\n",
        "                    'right_eye': right_eye,\n",
        "                    'confidence': 0.95,\n",
        "                    'method': 'MediaPipe',\n",
        "                    'eye_distance': eye_distance\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"MediaPipe error: {e}\")\n",
        "\n",
        "    # Méthode 2: Haar Cascade\n",
        "    try:\n",
        "        gray = cv2.cvtColor(img_for_detection, cv2.COLOR_RGB2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            # Prendre le plus grand visage\n",
        "            fx, fy, fw, fh = max(faces, key=lambda f: f[2] * f[3])\n",
        "\n",
        "            # Estimation anatomique des yeux\n",
        "            face_center_x = fx + fw / 2\n",
        "            eye_y = fy + fh * 0.37  # Position réaliste des yeux\n",
        "            eye_separation = fw * 0.25  # Distance réaliste\n",
        "\n",
        "            left_eye = np.array([face_center_x - eye_separation, eye_y])\n",
        "            right_eye = np.array([face_center_x + eye_separation, eye_y])\n",
        "\n",
        "            results.append({\n",
        "                'left_eye': left_eye,\n",
        "                'right_eye': right_eye,\n",
        "                'confidence': 0.75,\n",
        "                'method': 'Haar_Cascade',\n",
        "                'eye_distance': eye_separation * 2,\n",
        "                'face_rect': (fx, fy, fw, fh)\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Haar Cascade error: {e}\")\n",
        "\n",
        "    if not results:\n",
        "        return None\n",
        "\n",
        "    # Retourner le meilleur résultat\n",
        "    best = max(results, key=lambda x: x['confidence'])\n",
        "    return best\n",
        "\n",
        "def align_eyes_to_fixed_position(image_rgb, target_left, target_right, output_size=(1280, 720)):\n",
        "    \"\"\"\n",
        "    Aligne les yeux à des positions ABSOLUES sans zoom excessif\n",
        "    \"\"\"\n",
        "    detection = detect_face_and_eyes(image_rgb)\n",
        "\n",
        "    if detection is None:\n",
        "        print(\"❌ PAS DE VISAGE DÉTECTÉ - Image ignorée\")\n",
        "        return None\n",
        "\n",
        "    current_left = detection['left_eye']\n",
        "    current_right = detection['right_eye']\n",
        "    method = detection['method']\n",
        "\n",
        "    print(f\"👁️  Détection: {method}\")\n",
        "    print(f\"   Yeux actuels: L{current_left.astype(int)} R{current_right.astype(int)}\")\n",
        "\n",
        "    # Calculer les transformations nécessaires\n",
        "    current_center = (current_left + current_right) / 2.0\n",
        "    target_center = (np.array(target_left) + np.array(target_right)) / 2.0\n",
        "\n",
        "    # 1. ROTATION pour horizontaliser les yeux\n",
        "    current_vector = current_right - current_left\n",
        "    current_angle = math.atan2(current_vector[1], current_vector[0])\n",
        "    target_angle = math.atan2(target_right[1] - target_left[1], target_right[0] - target_left[0])\n",
        "    rotation_needed = target_angle - current_angle\n",
        "\n",
        "    print(f\"🔄 Rotation nécessaire: {math.degrees(rotation_needed):.2f}°\")\n",
        "\n",
        "    # 2. ÉCHELLE - Contrôlée et limitée\n",
        "    current_eye_distance = np.linalg.norm(current_vector)\n",
        "    target_eye_distance = np.linalg.norm(np.array(target_right) - np.array(target_left))\n",
        "    scale_factor = target_eye_distance / current_eye_distance if current_eye_distance > 0 else 1.0\n",
        "\n",
        "    # LIMITATION DU ZOOM - Éviter les zooms extrêmes\n",
        "    scale_factor = np.clip(scale_factor, 0.5, 2.0)  # Limiter entre 50% et 200%\n",
        "\n",
        "    print(f\"📏 Échelle appliquée: {scale_factor:.3f}\")\n",
        "\n",
        "    # 3. Appliquer la transformation étape par étape\n",
        "    h, w = image_rgb.shape[:2]\n",
        "\n",
        "    # Étape 1: Rotation autour du centre des yeux actuels\n",
        "    M_rot = cv2.getRotationMatrix2D(tuple(current_center), math.degrees(rotation_needed), scale_factor)\n",
        "    rotated = cv2.warpAffine(image_rgb, M_rot, (w, h),\n",
        "                            flags=cv2.INTER_LANCZOS4,\n",
        "                            borderMode=cv2.BORDER_CONSTANT,\n",
        "                            borderValue=(255, 255, 255))\n",
        "\n",
        "    # Calculer la nouvelle position des yeux après rotation\n",
        "    current_eyes_homog = np.array([[current_left[0], current_right[0]],\n",
        "                                   [current_left[1], current_right[1]],\n",
        "                                   [1, 1]])\n",
        "\n",
        "    M_rot_3x3 = np.vstack([M_rot, [0, 0, 1]])\n",
        "    rotated_eyes = M_rot_3x3 @ current_eyes_homog\n",
        "    new_center = np.mean(rotated_eyes[:2, :], axis=1)\n",
        "\n",
        "    # Étape 2: Translation vers la position cible\n",
        "    translation = target_center - new_center\n",
        "    print(f\"➡️  Translation: {translation.astype(int)}\")\n",
        "\n",
        "    M_trans = np.float32([[1, 0, translation[0]], [0, 1, translation[1]]])\n",
        "    final_result = cv2.warpAffine(rotated, M_trans, output_size,\n",
        "                                 flags=cv2.INTER_LANCZOS4,\n",
        "                                 borderMode=cv2.BORDER_CONSTANT,\n",
        "                                 borderValue=(255, 255, 255))\n",
        "\n",
        "    return final_result\n",
        "\n",
        "def create_aligned_slideshow(files, music=None, duration_per_face=4.0, fade_duration=1.5):\n",
        "    \"\"\"\n",
        "    Créer un diaporama avec alignement strict des yeux\n",
        "    \"\"\"\n",
        "    if not files:\n",
        "        return \"Aucune image fournie.\", None, None\n",
        "\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    clips = []\n",
        "\n",
        "    # POSITIONS FIXES ET ABSOLUES\n",
        "    output_size = (1280, 720)\n",
        "\n",
        "    # Position optimale pour les yeux (centre horizontal, légèrement haut)\n",
        "    center_x = output_size[0] // 2\n",
        "    eye_y = 250  # Position verticale\n",
        "    eye_separation = 140  # Distance entre les yeux\n",
        "\n",
        "    FIXED_LEFT_EYE = (center_x - eye_separation//2, eye_y)\n",
        "    FIXED_RIGHT_EYE = (center_x + eye_separation//2, eye_y)\n",
        "\n",
        "    print(f\"🎯 POSITIONS CIBLES FIXES:\")\n",
        "    print(f\"   Œil gauche: {FIXED_LEFT_EYE}\")\n",
        "    print(f\"   Œil droit: {FIXED_RIGHT_EYE}\")\n",
        "    print(f\"   Écartement: {eye_separation}px\")\n",
        "\n",
        "    successful_images = 0\n",
        "\n",
        "    for i, file in enumerate(files):\n",
        "        try:\n",
        "            print(f\"\\n📸 === TRAITEMENT IMAGE {i+1}/{len(files)} ===\")\n",
        "            print(f\"Fichier: {os.path.basename(file.name)}\")\n",
        "\n",
        "            # Charger l'image\n",
        "            pil_img = Image.open(file.name).convert('RGBA')\n",
        "\n",
        "            # Rotation légère pour effet vintage (très minime)\n",
        "            rotation = random.uniform(-0.5, 0.5)  # Rotation minimale\n",
        "            if rotation != 0:\n",
        "                pil_img = pil_img.rotate(rotation, expand=True, resample=Image.BICUBIC)\n",
        "\n",
        "            # Fond blanc\n",
        "            white_bg = Image.new(\"RGBA\", pil_img.size, (255, 255, 255, 255))\n",
        "            white_bg.paste(pil_img, (0, 0), pil_img)\n",
        "            white_bg = white_bg.convert('RGB')\n",
        "\n",
        "            # Bordure discrète\n",
        "            bordered_pil = ImageOps.expand(white_bg, border=10, fill='white')\n",
        "\n",
        "            # Conversion numpy\n",
        "            img_np = np.array(bordered_pil)\n",
        "\n",
        "            # ALIGNEMENT\n",
        "            aligned_img = align_eyes_to_fixed_position(\n",
        "                img_np,\n",
        "                FIXED_LEFT_EYE,\n",
        "                FIXED_RIGHT_EYE,\n",
        "                output_size\n",
        "            )\n",
        "\n",
        "            if aligned_img is not None:\n",
        "                # Créer le clip vidéo\n",
        "                clip = ImageClip(aligned_img).set_duration(duration_per_face)\n",
        "                clips.append(clip)\n",
        "                successful_images += 1\n",
        "\n",
        "                # Sauvegarder pour debug\n",
        "                debug_path = f'outputs/aligned_{i+1:02d}.jpg'\n",
        "                cv2.imwrite(debug_path, cv2.cvtColor(aligned_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                print(f\"✅ Image {i+1} alignée avec succès\")\n",
        "            else:\n",
        "                print(f\"⚠️  Image {i+1} IGNORÉE (pas de visage détecté)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur image {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not clips:\n",
        "        return \"❌ Aucune image avec visage détecté. Vérifiez vos images.\", None, None\n",
        "\n",
        "    if successful_images < len(files):\n",
        "        print(f\"⚠️  {len(files) - successful_images} image(s) ignorée(s) (pas de visage)\")\n",
        "\n",
        "    print(f\"\\n🎬 CRÉATION VIDÉO avec {successful_images} images\")\n",
        "\n",
        "    # Créer la vidéo finale\n",
        "    final_clip = concatenate_videoclips(clips, padding=-fade_duration, method=\"compose\")\n",
        "\n",
        "    # Ajouter la musique\n",
        "    if music is not None:\n",
        "        try:\n",
        "            audio = AudioFileClip(music.name).set_duration(final_clip.duration)\n",
        "            final_clip = final_clip.set_audio(audio)\n",
        "            print(\"🎵 Audio ajouté\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Erreur audio: {e}\")\n",
        "\n",
        "    # Sauvegarder\n",
        "    output_path = 'outputs/aligned_slideshow.mp4'\n",
        "    print(\"💾 Sauvegarde de la vidéo...\")\n",
        "\n",
        "    final_clip.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac',\n",
        "                              ffmpeg_params=['-pix_fmt', 'yuv420p'], verbose=False, logger=None)\n",
        "\n",
        "    final_clip.close()  # Libérer la mémoire\n",
        "\n",
        "    summary = f\"\"\"✅ VIDÉO CRÉÉE AVEC SUCCÈS!\n",
        "\n",
        "📊 Résumé:\n",
        "• Images traitées: {successful_images}/{len(files)}\n",
        "• Images ignorées: {len(files) - successful_images} (pas de visage)\n",
        "• Position des yeux: FIXE à {FIXED_LEFT_EYE} et {FIXED_RIGHT_EYE}\n",
        "• Zoom limité: Entre 50% et 200% pour éviter les déformations\n",
        "\n",
        "📁 Fichiers créés:\n",
        "• Vidéo finale: {output_path}\n",
        "• Images debug: outputs/aligned_XX.jpg\n",
        "\n",
        "💡 Conseil: Seules les images avec des visages clairement détectables sont incluses.\"\"\"\n",
        "\n",
        "    return summary, output_path, output_path\n",
        "\n",
        "# Interface Gradio optimisée\n",
        "iface = gr.Interface(\n",
        "    fn=create_aligned_slideshow,\n",
        "    inputs=[\n",
        "        gr.File(label=\"📸 Images (uniquement celles avec des visages)\",\n",
        "                file_types=[\"image\"],\n",
        "                file_count=\"multiple\"),\n",
        "        gr.File(label=\"🎵 Musique (optionnel)\",\n",
        "                file_types=[\"audio\"]),\n",
        "        gr.Slider(minimum=2.0, maximum=8.0, value=4.0, step=0.5,\n",
        "                 label=\"⏱️ Durée par image (secondes)\"),\n",
        "        gr.Slider(minimum=0.5, maximum=3.0, value=1.5, step=0.5,\n",
        "                 label=\"🔄 Durée transition (secondes)\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"📋 Rapport détaillé\", lines=10),\n",
        "        gr.Video(label=\"🎬 Vidéo finale\"),\n",
        "        gr.File(label=\"💾 Télécharger\")\n",
        "    ],\n",
        "    title=\"🎯 Alignement Parfait des Yeux - Version Robuste\",\n",
        "    description=\"\"\"\n",
        "    **ALIGNEMENT ULTRA-PRÉCIS avec GESTION INTELLIGENTE** 🎯\n",
        "\n",
        "    ✅ **Détection robuste**: Ignore automatiquement les images sans visage\n",
        "    ✅ **Positions absolues**: Yeux aux mêmes coordonnées exactes\n",
        "    ✅ **Zoom contrôlé**: Limité entre 50%-200% pour éviter les déformations\n",
        "    ✅ **Rotation précise**: Yeux parfaitement horizontaux\n",
        "    ✅ **Debug intégré**: Images de vérification sauvegardées\n",
        "\n",
        "    ⚠️ **Important**: Seules les images avec des visages clairement détectés seront incluses dans la vidéo finale.\n",
        "    \"\"\",\n",
        "    allow_flagging='never'\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(share=True, debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}