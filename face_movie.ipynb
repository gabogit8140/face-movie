{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui4F3zJ33aAO"
      },
      "execution_count": 10,
      "outputs": [],
      "source": [
        "!pip install --quiet mediapipe moviepy Pillow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kNa-MIHZ3aAQ",
        "outputId": "30b9d776-2b9d-4f9f-c0d6-9e427e874073"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d3c60d2989d74fc5b2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3c60d2989d74fc5b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 667, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 349, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2274, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1781, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4081309046.py\", line 48, in preview_auto\n",
            "    current_file = file.name\n",
            "                   ^^^^^^^^^\n",
            "AttributeError: 'list' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://d3c60d2989d74fc5b2.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import os, cv2, numpy as np, math, random\n",
        "import mediapipe as mp\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip\n",
        "import gradio as gr\n",
        "\n",
        "# === Init face detection ===\n",
        "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "validated_points = {}  # {filename: [(x_left,y_left), (x_right,y_right)]}\n",
        "current_file = None\n",
        "clicked_points = []\n",
        "\n",
        "# --- 1. Auto detect eyes ---\n",
        "def detect_face_and_eyes(image_rgb):\n",
        "    h, w, _ = image_rgb.shape\n",
        "    results = []\n",
        "    try:\n",
        "        res = mp_face_mesh.process(cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB))\n",
        "        if res and res.multi_face_landmarks:\n",
        "            lm = res.multi_face_landmarks[0].landmark\n",
        "            left_eye = np.mean([(lm[33].x*w, lm[33].y*h), (lm[133].x*w, lm[133].y*h)], axis=0)\n",
        "            right_eye = np.mean([(lm[362].x*w, lm[362].y*h), (lm[263].x*w, lm[263].y*h)], axis=0)\n",
        "            results.append({\"left_eye\": left_eye, \"right_eye\": right_eye, \"confidence\": 0.95})\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100,100))\n",
        "        if len(faces)>0:\n",
        "            fx, fy, fw, fh = max(faces, key=lambda f: f[2]*f[3])\n",
        "            cx = fx+fw/2; eye_y = fy+fh*0.37; sep = fw*0.25\n",
        "            results.append({\"left_eye\":[cx-sep,eye_y], \"right_eye\":[cx+sep,eye_y], \"confidence\":0.75})\n",
        "    except:\n",
        "        pass\n",
        "    if not results: return None\n",
        "    return max(results, key=lambda x:x[\"confidence\"])\n",
        "\n",
        "# --- 2. Preview auto detection ---\n",
        "def preview_auto(file):\n",
        "    global current_file, clicked_points\n",
        "    current_file = file.name\n",
        "    clicked_points = []\n",
        "    pil_img = Image.open(file.name).convert(\"RGB\")\n",
        "    img_np = np.array(pil_img)\n",
        "    det = detect_face_and_eyes(img_np)\n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    auto_points = []\n",
        "    if det:\n",
        "        auto_points = [(int(det[\"left_eye\"][0]), int(det[\"left_eye\"][1])),\n",
        "                       (int(det[\"right_eye\"][0]), int(det[\"right_eye\"][1]))]\n",
        "        for p in auto_points:\n",
        "            draw.ellipse([p[0]-5,p[1]-5,p[0]+5,p[1]+5], outline=\"red\", width=3)\n",
        "    return pil_img, str(auto_points)\n",
        "\n",
        "# --- 3. Handle clicks on image ---\n",
        "def on_click(evt: gr.SelectData):\n",
        "    global clicked_points, current_file\n",
        "    clicked_points.append(evt.index)\n",
        "    if len(clicked_points) > 2:\n",
        "        clicked_points = clicked_points[-2:]\n",
        "    return str(clicked_points)\n",
        "\n",
        "# --- 4. Validate selection ---\n",
        "def validate_points():\n",
        "    global current_file, clicked_points, validated_points\n",
        "    if current_file and len(clicked_points)==2:\n",
        "        validated_points[current_file] = clicked_points.copy()\n",
        "        return f\"‚úÖ {os.path.basename(current_file)} valid with {clicked_points}\"\n",
        "    return \"‚ö†Ô∏è Please select exactly 2 points (left & right eyes).\"\n",
        "\n",
        "# --- 5. Align image ---\n",
        "def align_eyes_to_fixed_position(image_rgb, eyes, target_left, target_right, output_size=(1024,768)):\n",
        "    left, right = np.array(eyes[0]), np.array(eyes[1])\n",
        "    center = (left+right)/2.0\n",
        "    t_center = (np.array(target_left)+np.array(target_right))/2.0\n",
        "    angle = math.atan2((right-left)[1], (right-left)[0])\n",
        "    t_angle = math.atan2((target_right-target_left)[1], (target_right-target_left)[0])\n",
        "    rot = t_angle-angle\n",
        "    scale=1.0\n",
        "    h,w=image_rgb.shape[:2]\n",
        "    M=cv2.getRotationMatrix2D(tuple(center), math.degrees(rot), scale)\n",
        "    rotated=cv2.warpAffine(image_rgb,M,(w,h),flags=cv2.INTER_LANCZOS4,borderValue=(255,255,255))\n",
        "    new_c=np.dot(M,[center[0],center[1],1])\n",
        "    trans=t_center-new_c\n",
        "    M2=np.float32([[1,0,trans[0]],[0,1,trans[1]]])\n",
        "    return cv2.warpAffine(rotated,M2,output_size,flags=cv2.INTER_LANCZOS4,borderValue=(255,255,255))\n",
        "\n",
        "# --- 6. Slideshow ---\n",
        "def create_slideshow(files, music=None, dur=4.0, fade=2.0):\n",
        "    os.makedirs(\"outputs\", exist_ok=True)\n",
        "    clips=[]; out_size=(1024,768)\n",
        "    cx=out_size[0]//2; eye_y=out_size[1]//3; sep=140\n",
        "    FIXED_L=(cx-sep//2,eye_y); FIXED_R=(cx+sep//2,eye_y)\n",
        "    for f in files:\n",
        "        if f.name not in validated_points: continue\n",
        "        pil_img=Image.open(f.name).convert(\"RGB\")\n",
        "        white_bg=Image.new(\"RGB\", pil_img.size,(255,255,255)); white_bg.paste(pil_img,(0,0))\n",
        "        bordered=ImageOps.expand(white_bg,border=30,fill=\"white\")\n",
        "        aligned=align_eyes_to_fixed_position(np.array(bordered), validated_points[f.name], FIXED_L,FIXED_R,out_size)\n",
        "        rotation=random.uniform(-4,4)\n",
        "        clip=ImageClip(aligned).set_duration(dur).rotate(rotation)\n",
        "        clips.append(clip)\n",
        "    if not clips: return \"‚ùå No validated images\", None, None\n",
        "    final=concatenate_videoclips(clips,method=\"compose\",padding=-fade).crossfadein(fade)\n",
        "    if music:\n",
        "        try:\n",
        "            audio=AudioFileClip(music.name).set_duration(final.duration)\n",
        "            final=final.set_audio(audio)\n",
        "        except: pass\n",
        "    out=\"outputs/final_slideshow.mp4\"\n",
        "    final.write_videofile(out,fps=24,codec=\"libx264\",audio_codec=\"aac\",\n",
        "                          ffmpeg_params=[\"-pix_fmt\",\"yuv420p\"],verbose=False,logger=None)\n",
        "    final.close()\n",
        "    return f\"‚úÖ Video created with {len(clips)} validated images\", out, out\n",
        "\n",
        "# === Gradio UI ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üëÅÔ∏è Validate eyes before slideshow\")\n",
        "    with gr.Row():\n",
        "        file_in=gr.File(file_types=[\"image\"], file_count=\"multiple\", label=\"üì∏ Photos\")\n",
        "        music_in=gr.File(file_types=[\"audio\"], label=\"üéµ Music (optional)\")\n",
        "    preview_btn=gr.Button(\"üîç Preview first photo\")\n",
        "    img=gr.Image(type=\"pil\", interactive=True, label=\"Click 2 points (eyes)\")\n",
        "    log=gr.Textbox(label=\"Log\")\n",
        "    preview_btn.click(preview_auto, inputs=[file_in], outputs=[img, log])\n",
        "    img.select(on_click, outputs=log)\n",
        "    validate_btn=gr.Button(\"‚úÖ Validate this photo\")\n",
        "    validate_btn.click(validate_points, outputs=log)\n",
        "    dur=gr.Slider(2,8,4,0.5,label=\"Duration per image (s)\")\n",
        "    fade=gr.Slider(1,4,2,0.5,label=\"Fade duration (s)\")\n",
        "    gen_btn=gr.Button(\"üé¨ Generate video\")\n",
        "    out_txt=gr.Textbox()\n",
        "    out_vid=gr.Video()\n",
        "    out_file=gr.File()\n",
        "    gen_btn.click(create_slideshow, inputs=[file_in,music_in,dur,fade], outputs=[out_txt,out_vid,out_file])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}