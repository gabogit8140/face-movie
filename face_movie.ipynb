{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install --quiet gradio opencv-python moviepy numpy mediapipe Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import mediapipe as mp\n",
        "from PIL import Image, ImageOps\n",
        "from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip\n",
        "import random\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def align_face(image_rgb, output_size=(1280,720), eye_target=(640,300)):\n",
        "    h, w, _ = image_rgb.shape\n",
        "    img = image_rgb.astype(np.uint8)\n",
        "    res = mp_face_mesh.process(img)\n",
        "    left_eye = None\n",
        "    right_eye = None\n",
        "    if res and res.multi_face_landmarks:\n",
        "        lm = res.multi_face_landmarks[0].landmark\n",
        "        left_idxs = [33, 133]\n",
        "        right_idxs = [362, 263]\n",
        "        left_eye = np.mean([(lm[i].x * w, lm[i].y * h) for i in left_idxs], axis=0)\n",
        "        right_eye = np.mean([(lm[i].x * w, lm[i].y * h) for i in right_idxs], axis=0)\n",
        "    if left_eye is None or right_eye is None:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "        if len(faces) == 0:\n",
        "            return None\n",
        "        x, y, wb, hb = max(faces, key=lambda f: f[2]*f[3])\n",
        "        cx = x + wb/2\n",
        "        cy = y + hb/3.0\n",
        "        left_eye = np.array([cx - wb*0.18, cy])\n",
        "        right_eye = np.array([cx + wb*0.18, cy])\n",
        "    left_eye = np.array(left_eye, dtype=np.float32)\n",
        "    right_eye = np.array(right_eye, dtype=np.float32)\n",
        "    eyes_center = (left_eye + right_eye) / 2.0\n",
        "    dx, dy = right_eye[0] - left_eye[0], right_eye[1] - left_eye[1]\n",
        "    current_eye_dist = np.hypot(dx, dy)\n",
        "    if current_eye_dist < 1.0:\n",
        "        return None\n",
        "    angle = np.degrees(np.arctan2(dy, dx))\n",
        "    scale = 1.0\n",
        "    cx, cy = eyes_center\n",
        "    M = cv2.getRotationMatrix2D((cx, cy), angle, scale)\n",
        "    eyes_center_rot = np.dot(M, np.array([cx, cy, 1.0]))\n",
        "    tx, ty = eye_target[0] - eyes_center_rot[0], eye_target[1] - eyes_center_rot[1]\n",
        "    M[0,2] += tx\n",
        "    M[1,2] += ty\n",
        "    out_w, out_h = output_size\n",
        "    warped = cv2.warpAffine(img, M, (out_w, out_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
        "    return warped\n",
        "\n",
        "def add_border_and_tilt(img_np):\n",
        "    pil = Image.fromarray(img_np)\n",
        "    bordered = ImageOps.expand(pil, border=30, fill='white')\n",
        "    rot_angle = random.uniform(-5,5)\n",
        "    return np.array(bordered.rotate(rot_angle, expand=True, fillcolor='white'))\n",
        "\n",
        "def create_face_movie(files, music=None, duration_per_face=3.0, fade_duration=1.5):\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    clips = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            pil = Image.open(f.name).convert('RGB')\n",
        "            img = np.array(pil)\n",
        "        except:\n",
        "            continue\n",
        "        aligned = align_face(img)\n",
        "        if aligned is None:\n",
        "            continue\n",
        "        stacked = add_border_and_tilt(aligned)\n",
        "        clip = ImageClip(stacked).set_duration(duration_per_face)\n",
        "        clips.append(clip)\n",
        "    if not clips:\n",
        "        return \"Aucune image utilisable\", None, None\n",
        "    final = clips[0]\n",
        "    for next_clip in clips[1:]:\n",
        "        final = concatenate_videoclips([final.crossfadeout(fade_duration), next_clip.crossfadein(fade_duration)], method='compose')\n",
        "    if music is not None:\n",
        "        try:\n",
        "            audio = AudioFileClip(music.name).set_duration(final.duration)\n",
        "            final = final.set_audio(audio)\n",
        "        except:\n",
        "            pass\n",
        "    out_path = 'outputs/face_movie_stack.mov'\n",
        "    final.write_videofile(out_path, fps=24, codec='libx264', audio_codec='aac', ffmpeg_params=['-profile:v', 'baseline', '-pix_fmt', 'yuv420p'])\n",
        "    return f\"Vidéo générée avec {len(clips)} images\", out_path, out_path\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=create_face_movie,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Images (multiple)\", file_types=[\"image\"], file_count=\"multiple\"),\n",
        "        gr.File(label=\"Musique (optionnel)\", file_types=[\"audio\"])\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Statut\", lines=5),\n",
        "        gr.Video(label=\"Vidéo\"),\n",
        "        gr.File(label=\"Télécharger\")\n",
        "    ],\n",
        "    title=\"Face Movie Stack — 16:9 + bordures + empilement\",\n",
        "    description=\"Photos alignées sur les yeux, format 16:9, empilement avec bordure blanche et fondu entre images\"\n",
        ")\n",
        "iface.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}